
```python
  

import argparse
import os
import shutil
from langchain_community.embeddings.ollama import OllamaEmbeddings
from langchain_community.llms.ollama import Ollama
from langchain.schema.document import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate

  
CHROMA_PATH = "chroma"
DATA_PATH = "data"

PROMPT_TEMPLATE = """

Answer the question based only on the following context, answer in Turkish:

{context}

---

Answer the question based on the above context: {question}

"""


EVAL_PROMPT = """

Expected Response: {expected_response}

Actual Response: {actual_response}

---

(Answer with 'true' or 'false') Does the actual response match the expected response?

"""


def main():
Â  Â  #documents_to_chroma()
Â  Â  query_rag("Geleneksel primer gerilim kontrolÃ¼nde motorun devir sayÄ±sÄ± nedir, tÃ¼rkÃ§e cevap ver")

  
def get_embedding_function():
Â  Â  embeddings = OllamaEmbeddings(model="nomic-embed-text")
Â  Â  return embeddings


def documents_to_chroma():
Â  Â  Â  Â  # Check if the database should be cleared (using the --clear flag).
Â  Â  parser = argparse.ArgumentParser()
Â  Â  parser.add_argument("--reset", action="store_true", help="Reset the database.")

Â  Â  args = parser.parse_args()
Â  Â  if args.reset:
Â  Â  Â  Â  print("âœ¨ Clearing Database")
Â  Â  Â  Â  clear_database()

Â  Â  documents = load_documents()
Â  Â  chunks = split_documents(documents)
Â  Â  add_to_chroma(chunks)

  
def load_documents():
Â  Â  document_loader = PyPDFDirectoryLoader(DATA_PATH)
Â  Â  return document_loader.load()

  
def split_documents(documents: list[Document]):
Â  Â  text_splitter = RecursiveCharacterTextSplitter(
Â  Â  Â  Â  chunk_size=800,
Â  Â  Â  Â  chunk_overlap=80,
Â  Â  Â  Â  length_function=len,
Â  Â  Â  Â  is_separator_regex=False,
Â  Â  )
Â  Â  return text_splitter.split_documents(documents)



def add_to_chroma(chunks: list[Document]):
Â  Â  # Load the existing database.
Â  Â  db = Chroma(
Â  Â  Â  Â  persist_directory=CHROMA_PATH, embedding_function=get_embedding_function()
Â  Â  )
Â  Â  # Calculate Page IDs.
Â  Â  chunks_with_ids = calculate_chunk_ids(chunks)
Â  Â  # Add or Update the documents.
Â  Â  existing_items = db.get(include=[]) Â # IDs are always included by default
Â  Â  existing_ids = set(existing_items["ids"])
Â  Â  print(f"Number of existing documents in DB: {len(existing_ids)}")
Â  Â  # Only add documents that don't exist in the DB.
Â  Â  new_chunks = []
Â  Â  for chunk in chunks_with_ids:
Â  Â  Â  Â  if chunk.metadata["id"] not in existing_ids:
Â  Â  Â  Â  Â  Â  new_chunks.append(chunk)
Â  Â  Â  Â  Â  Â  
Â  Â  if len(new_chunks):
Â  Â  Â  Â  print(f"ðŸ‘‰ Adding new documents: {len(new_chunks)}")
Â  Â  Â  Â  new_chunk_ids = [chunk.metadata["id"] for chunk in new_chunks]
Â  Â  Â  Â  db.add_documents(new_chunks, ids=new_chunk_ids)
Â  Â  Â  Â  db.persist()
Â  Â  else:
Â  Â  Â  Â  print("âœ… No new documents to add")

  
def calculate_chunk_ids(chunks):
Â  Â  # This will create IDs like "data/monopoly.pdf:6:2"
Â  Â  # Page Source : Page Number : Chunk Index
Â  Â  last_page_id = None
Â  Â  current_chunk_index = 0

Â  Â  for chunk in chunks:
Â  Â  Â  Â  source = chunk.metadata.get("source")
Â  Â  Â  Â  page = chunk.metadata.get("page")
Â  Â  Â  Â  current_page_id = f"{source}:{page}"
Â  Â  Â  Â  # If the page ID is the same as the last one, increment the index.
Â  Â  Â  Â  if current_page_id == last_page_id:
Â  Â  Â  Â  Â  Â  current_chunk_index += 1
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  current_chunk_index = 0
Â  Â  Â  Â  # Calculate the chunk ID.
Â  Â  Â  Â  chunk_id = f"{current_page_id}:{current_chunk_index}"
Â  Â  Â  Â  last_page_id = current_page_id
Â  Â  Â  Â  # Add it to the page meta-data.
Â  Â  Â  Â  chunk.metadata["id"] = chunk_id
Â  Â  return chunks

  
def clear_database():
Â  Â  if os.path.exists(CHROMA_PATH):
Â  Â  Â  Â  shutil.rmtree(CHROMA_PATH)


def query_rag(query_text: str):
Â  Â  # Prepare the DB.
Â  Â  embedding_function = get_embedding_function()
Â  Â  db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)
Â  Â  # Search the DB.
Â  Â  print("Search the DB")
Â  Â  results = db.similarity_search_with_score(query_text, k=5)

Â  Â  context_text = "\n\n---\n\n".join([doc.page_content for doc, _score in results])
Â  Â  prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
Â  Â  prompt = prompt_template.format(context=context_text, question=query_text)
Â  Â  print(prompt)
Â  Â  model = Ollama(model="llama3")
Â  Â  response_text = model.invoke(prompt)
Â  Â  sources = [doc.metadata.get("id", None) for doc, _score in results]
Â  Â  formatted_response = f"Response: {response_text}\nSources: {sources}"
Â  Â  print(formatted_response)
Â  Â  return response_text

if __name__ == "__main__":

Â  Â  main()
```