{
  "main": {
    "id": "13394e9b09e00512",
    "type": "split",
    "children": [
      {
        "id": "f99d2ec1edd7ee81",
        "type": "tabs",
        "children": [
          {
            "id": "b674564b4ea50d2a",
            "type": "leaf",
            "state": {
              "type": "graph",
              "state": {}
            }
          },
          {
            "id": "88f32f99516f2032",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "mycodes/rag-tutorial-v2-main/README.md",
                "mode": "source",
                "source": false
              }
            }
          },
          {
            "id": "32c9142a3fb4984f",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "mycodes/rag-tutorial-v2-main/code_understanding rag-tutorial.md",
                "mode": "source",
                "source": false
              }
            }
          },
          {
            "id": "6ac80092f23ef807",
            "type": "leaf",
            "state": {
              "type": "graph",
              "state": {}
            }
          },
          {
            "id": "23c7d6d58b717d54",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "what is llama cpp.md",
                "mode": "source",
                "source": false
              }
            }
          },
          {
            "id": "6ffec095dcf510f8",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "Llama3 RAG on Google Colab.md",
                "mode": "source",
                "source": false
              }
            }
          },
          {
            "id": "97d29234db7c4e1a",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "Llama 3 and Langchain Locally.md",
                "mode": "preview",
                "source": false
              }
            }
          }
        ],
        "currentTab": 6
      }
    ],
    "direction": "vertical"
  },
  "left": {
    "id": "f7ec28e28a295754",
    "type": "split",
    "children": [
      {
        "id": "d5718a79b90c2504",
        "type": "tabs",
        "children": [
          {
            "id": "b3822fac48e363cb",
            "type": "leaf",
            "state": {
              "type": "file-explorer",
              "state": {
                "sortOrder": "alphabetical"
              }
            }
          },
          {
            "id": "2c5bad05ed903e10",
            "type": "leaf",
            "state": {
              "type": "search",
              "state": {
                "query": "tag:#LLM",
                "matchingCase": false,
                "explainSearch": false,
                "collapseAll": false,
                "extraContext": false,
                "sortOrder": "alphabetical"
              }
            }
          },
          {
            "id": "db35aabbdce2f03d",
            "type": "leaf",
            "state": {
              "type": "bookmarks",
              "state": {}
            }
          }
        ]
      }
    ],
    "direction": "horizontal",
    "width": 294.5
  },
  "right": {
    "id": "92689ae3ba8b9034",
    "type": "split",
    "children": [
      {
        "id": "2537fbaa2a4d1c0e",
        "type": "tabs",
        "children": [
          {
            "id": "8f322322becd913a",
            "type": "leaf",
            "state": {
              "type": "backlink",
              "state": {
                "file": "Llama 3 and Langchain Locally.md",
                "collapseAll": false,
                "extraContext": false,
                "sortOrder": "alphabetical",
                "showSearch": false,
                "searchQuery": "",
                "backlinkCollapsed": false,
                "unlinkedCollapsed": true
              }
            }
          },
          {
            "id": "16c40cf131f845c2",
            "type": "leaf",
            "state": {
              "type": "outgoing-link",
              "state": {
                "file": "Llama 3 and Langchain Locally.md",
                "linksCollapsed": false,
                "unlinkedCollapsed": true
              }
            }
          },
          {
            "id": "954d5d35f238b15d",
            "type": "leaf",
            "state": {
              "type": "tag",
              "state": {
                "sortOrder": "frequency",
                "useHierarchy": false
              }
            }
          },
          {
            "id": "fcd08ee02d43e834",
            "type": "leaf",
            "state": {
              "type": "outline",
              "state": {
                "file": "Llama 3 and Langchain Locally.md"
              }
            }
          }
        ],
        "currentTab": 2
      }
    ],
    "direction": "horizontal",
    "width": 200
  },
  "left-ribbon": {
    "hiddenItems": {
      "switcher:Open quick switcher": false,
      "graph:Open graph view": false,
      "canvas:Create new canvas": false,
      "daily-notes:Open today's daily note": false,
      "templates:Insert template": false,
      "command-palette:Open command palette": false,
      "obsidian-excalidraw-plugin:Create new drawing": false
    }
  },
  "active": "97d29234db7c4e1a",
  "lastOpenFiles": [
    "mycodes/rag-tutorial-v2-main/llama_index_ornk-1.py",
    "mycodes/rag-tutorial-v2-main/sil1 copy.py",
    "mycodes/rag-tutorial-v2-main/myllamaindex/image__vector_store.json",
    "mycodes/rag-tutorial-v2-main/myllamaindex/default__vector_store.json",
    "mycodes/rag-tutorial-v2-main/myllamaindex/graph_store.json",
    "mycodes/rag-tutorial-v2-main/myllamaindex/index_store.json",
    "mycodes/rag-tutorial-v2-main/myllamaindex/docstore.json",
    "mycodes/rag-tutorial-v2-main/myllamaindex",
    "Llama3 RAG on Google Colab.md",
    "Llama 3 and Langchain Locally.md",
    "what is llama cpp.md",
    "mycodes/rag-tutorial-v2-main/code_understanding rag-tutorial.md",
    "mycodes/rag-tutorial-v2-main/README.md",
    "mycodes/rag-tutorial-v2-main/langchain_sentence_transformers-1.py",
    "mycodes/rag-tutorial-v2-main/chroma/chroma.sqlite3-journal",
    "mycodes/rag-tutorial-v2-main/data/ffff.pdf",
    "A Deep Dive into LangChain vs. LlamaIndex for Data Indexing and Retrieval.md",
    "chunks and  adding chunk_id every chunk.md",
    "langchain vs llamaindex.md",
    "Llama3 is a transformer model.md",
    "llm lerde rag nedir.md",
    "mermaid-examples.md",
    "Model Architecture.md",
    "Retrieval Augmented Generation (RAG) app that operates entirely offline.md",
    "The ABCs of AI Transformers, Tokens, and Embeddings.md",
    "what is chroma client.md",
    "Git Clone disable TLS SSL verification for a single git command.md",
    "Vector Store/Key Components of a Vector Store Data.md",
    "langchain/LangChain Text Splitters.md",
    "langchain/langchain.embeddings.ollama.md",
    "Vector Store/Chroma/open-source vector store.md",
    "huggingface/sentence-transformers.md",
    "Ollama/Ollama vs OllamaEmbeddings.md",
    "tokenizer/in sentence_transformers ,you don't see tokenizers explicitly because.md",
    "langchain/Embeddings.md",
    "langchain/Build a Retrieval Augmented Generation (RAG) App.md"
  ]
}